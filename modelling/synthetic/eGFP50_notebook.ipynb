{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af16c4d",
   "metadata": {},
   "source": [
    "# eGFP50 model â€” training notebook\n",
    "\n",
    "Converted from `eGFP50_model.py`. This notebook contains the full pipeline to train a CNN+BiLSTM+Attention model\n",
    "to predict mean ribosome load (rl) from 5' UTR sequence and structure (egfp_50 dataset). It is annotated with\n",
    "explanations and split into runnable cells for interactive use.\n",
    "\n",
    "**Author:** Mike Wang\n",
    "\n",
    "**Notes**:\n",
    "- Update DATA_PATH and MODEL_OUT_PATH in the Configuration cell before running.\n",
    "- Long-running cells (model training) may take a long time depending on hardware; consider working with a subset for quick testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85491b4f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit file paths and hyperparameters below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / constants\n",
    "DATA_PATH = \"~/data/GSM3130435_egfp_unmod_1_structure_feature_table_maxBPspan_30.txt\"\n",
    "MODEL_OUT_PATH = \"~/models/synthetic/model_eGFP_50.h5\"\n",
    "MAX_ROWS = 280_000\n",
    "SEQ_LEN = 80\n",
    "RANDOM_STATE = 2021\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 999\n",
    "\n",
    "# DATA_PATH, MODEL_OUT_PATH, MAX_ROWS, SEQ_LEN, RANDOM_STATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd567fcd",
   "metadata": {},
   "source": [
    "## Imports and environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb76571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and logging\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr, linregress\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# reproducibility\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87978391",
   "metadata": {},
   "source": [
    "## One-hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pd.DataFrame, seqcol: str = \"sequence\", strucol: str = \"structure_full\", seq_len: int = SEQ_LEN):\n",
    "    \"\"\"\n",
    "    One-hot encode sequences and sequence+structure for model input.\n",
    "    - seq_vectors: (N, seq_len, 4) order [A, C, G, T]\n",
    "    - stru_vectors: (N, seq_len, 12) combinations of base + structure char (A(, A), A., ...)\n",
    "    Unknown characters and padding remain zeros.\n",
    "    \"\"\"\n",
    "    nuc_d: Dict[str, list] = {\n",
    "        \"A\": [1, 0, 0, 0],\n",
    "        \"C\": [0, 1, 0, 0],\n",
    "        \"G\": [0, 0, 1, 0],\n",
    "        \"T\": [0, 0, 0, 1],\n",
    "        \"U\": [0, 0, 0, 1],\n",
    "        \"N\": [0, 0, 0, 0],\n",
    "    }\n",
    "\n",
    "    bases = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    struct_symbols = [\"(\", \")\", \".\"]\n",
    "    stru_d = {}\n",
    "    idx = 0\n",
    "    for b in bases:\n",
    "        for s in struct_symbols:\n",
    "            vec = [1 if i == idx else 0 for i in range(12)]\n",
    "            stru_d[f\"{b}{s}\"] = vec\n",
    "            idx += 1\n",
    "    fallback_12 = [0] * 12\n",
    "\n",
    "    N = len(df)\n",
    "    seq_vectors = np.zeros((N, seq_len, 4), dtype=np.float32)\n",
    "    stru_vectors = np.zeros((N, seq_len, 12), dtype=np.float32)\n",
    "\n",
    "    seq_series = df[seqcol].fillna(\"\").astype(str).str.upper()\n",
    "    stru_series = df[strucol].fillna(\"\").astype(str).str.upper()\n",
    "\n",
    "    for i, (seq, stru) in enumerate(zip(seq_series, stru_series)):\n",
    "        seq = seq[:seq_len]\n",
    "        stru = stru[:seq_len]\n",
    "        for j, ch in enumerate(seq):\n",
    "            seq_vectors[i, j, :] = nuc_d.get(ch, [0, 0, 0, 0])\n",
    "        for j in range(seq_len):\n",
    "            if j < len(seq) and j < len(stru):\n",
    "                key = seq[j] + stru[j]\n",
    "                stru_vectors[i, j, :] = stru_d.get(key, fallback_12)\n",
    "    return seq_vectors, stru_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d82dcb",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff13aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Attention layer returning (context_vector, attention_weights).\n",
    "    \"\"\"\n",
    "    def __init__(self, bias: bool = True, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(\"glorot_uniform\")\n",
    "        self.bias = bias\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.features_dim = 0\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],), initializer=self.init, name=f\"{self.name}_W\",\n",
    "                                 regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(1,), initializer=\"zeros\", name=f\"{self.name}_b\",\n",
    "                                     regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = tf.shape(x)[1]\n",
    "        flat_x = tf.reshape(x, (-1, features_dim))\n",
    "        e = tf.reshape(tf.matmul(flat_x, tf.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            e = e + self.b\n",
    "        e = tf.tanh(e)\n",
    "        a = tf.exp(e)\n",
    "        if mask is not None:\n",
    "            a *= tf.cast(mask, tf.float32)\n",
    "        a /= tf.cast(tf.reduce_sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(), tf.float32)\n",
    "        a_expanded = tf.expand_dims(a, axis=-1)\n",
    "        c = tf.reduce_sum(a_expanded * x, axis=1)\n",
    "        return c, a_expanded\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.features_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954af94",
   "metadata": {},
   "source": [
    "## Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen: int = SEQ_LEN, hidden_dim: int = 64, dropout: float = 0.3, dropout_rate: float = 0.2):\n",
    "    input_stru = L.Input(shape=(maxlen, 12), name=\"Input_stru\")\n",
    "    input_seq = L.Input(shape=(maxlen, 4), name=\"Input_seq\")\n",
    "    input_mfe = L.Input(shape=(1,), name=\"Input_mfe\")\n",
    "    # Structure branch\n",
    "    s = L.Conv1D(32, 1, padding=\"same\", name=\"Conv1D_stru\")(input_stru)\n",
    "    s = L.BatchNormalization()(s)\n",
    "    s = L.ReLU()(s)\n",
    "    s_branch1 = L.Conv1D(64, 7, padding=\"same\", name=\"Conv1D_stru_feature2\")(s)\n",
    "    s_branch1 = L.BatchNormalization()(s_branch1)\n",
    "    s_branch1 = L.ReLU()(s_branch1)\n",
    "    s_branch2 = L.Conv1D(64, 9, padding=\"same\", name=\"Conv1D_stru_feature3\")(s)\n",
    "    s_branch2 = L.BatchNormalization()(s_branch2)\n",
    "    s_branch2 = L.ReLU()(s_branch2)\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_stru2\")(s_branch1)\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_stru2_1\")(lstm_s1)\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_stru3\")(s_branch2)\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_stru3_1\")(lstm_s2)\n",
    "    merged_s = L.concatenate([lstm_s1, lstm_s2], axis=-1)\n",
    "    merged_s = L.Conv1D(256, 1, padding=\"same\", name=\"Conv1D_stru_merged\")(merged_s)\n",
    "    merged_s = L.BatchNormalization()(merged_s)\n",
    "    merged_s = L.ReLU()(merged_s)\n",
    "    stru_out, attention_weight_stru = Attention(name=\"Attention_stru\")(merged_s)\n",
    "    stru_out = L.Dropout(0.3)(stru_out)\n",
    "    # Sequence branch\n",
    "    q = L.Conv1D(32, 1, padding=\"same\", name=\"Conv1D_seq\")(input_seq)\n",
    "    q = L.BatchNormalization()(q)\n",
    "    q = L.ReLU()(q)\n",
    "    q1 = L.Conv1D(64, 3, padding=\"same\", name=\"Conv1D_seq_feature1\")(q)\n",
    "    q1 = L.BatchNormalization()(q1)\n",
    "    q1 = L.ReLU()(q1)\n",
    "    q2 = L.Conv1D(64, 5, padding=\"same\", name=\"Conv1D_seq_feature2\")(q)\n",
    "    q2 = L.BatchNormalization()(q2)\n",
    "    q2 = L.ReLU()(q2)\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_seq\")(q1)\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_seq1\")(lstm_q1)\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_seq2\")(q2)\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer=\"orthogonal\"), name=\"LSTM_seq2_1\")(lstm_q2)\n",
    "    merged_q = L.concatenate([lstm_q1, lstm_q2], axis=-1)\n",
    "    merged_q = L.Conv1D(256, 1, padding=\"same\", name=\"Conv1D_seq_merged\")(merged_q)\n",
    "    merged_q = L.BatchNormalization()(merged_q)\n",
    "    merged_q = L.ReLU()(merged_q)\n",
    "    seq_out, attention_weight_seq = Attention(name=\"Attention_seq\")(merged_q)\n",
    "    seq_out = L.Dropout(0.3)(seq_out)\n",
    "    # Final head\n",
    "    x_flat = L.Concatenate()([stru_out, seq_out, input_mfe])\n",
    "    x = L.Dense(256, activation=\"relu\")(x_flat)\n",
    "    x = L.Dropout(0.5)(x)\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    out = L.Dense(1, activation=\"linear\", name=\"out\")(x)\n",
    "    model = tf.keras.Model(inputs=[input_stru, input_seq, input_mfe], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b349d",
   "metadata": {},
   "source": [
    "## Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_from_slope(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"Compute r-squared using scipy.linregress (same as original script).\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(y_true, y_pred)\n",
    "    return r_value ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f7f56",
   "metadata": {},
   "source": [
    "## Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "logger.info(\"Loading data from %s\", DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.iloc[:MAX_ROWS].copy()\n",
    "logger.info(\"Data shape after slicing: %s\", df.shape)\n",
    "\n",
    "# Train/test split consistent with original script\n",
    "e_test = df.iloc[:20000].copy()\n",
    "e_train = df.iloc[20000:].copy()\n",
    "logger.info(\"Train / test split sizes: %d / %d\", len(e_train), len(e_test))\n",
    "\n",
    "# One-hot encoding for train\n",
    "logger.info(\"One-hot encoding train sequences (seq_len=%d)\", SEQ_LEN)\n",
    "seq_e_train, stru_e_train = one_hot_encode(e_train, seqcol=\"sequence\", strucol=\"structure_full\", seq_len=SEQ_LEN)\n",
    "\n",
    "# Norm_mfe and scaling\n",
    "e_train[\"Norm_mfe\"] = e_train[\"MFE_full\"] / 80.0\n",
    "e_test[\"Norm_mfe\"] = e_test[\"MFE_full\"] / 80.0\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train_meta = mm_scaler.fit_transform(e_train[[\"Norm_mfe\"]].values)\n",
    "scaled_test_meta = mm_scaler.transform(e_test[[\"Norm_mfe\"]].values)\n",
    "\n",
    "# Scale target rl on training set\n",
    "target_scaler = StandardScaler()\n",
    "e_train[\"scaled_rl\"] = target_scaler.fit_transform(e_train[\"rl\"].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Build model\n",
    "logger.info(\"Building model\")\n",
    "model = build_model(maxlen=SEQ_LEN)\n",
    "model.summary()\n",
    "\n",
    "# Train/validation split\n",
    "X_train_seq, X_val_seq, X_train_stru, X_val_stru, X_train_meta, X_val_meta, y_train, y_val = train_test_split(\n",
    "    seq_e_train, stru_e_train, scaled_train_meta, e_train[\"scaled_rl\"].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    [X_train_stru, X_train_seq, X_train_meta],\n",
    "    y_train,\n",
    "    validation_data=([X_val_stru, X_val_seq, X_val_meta], y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "os.makedirs(os.path.dirname(MODEL_OUT_PATH), exist_ok=True)\n",
    "model.save(MODEL_OUT_PATH)\n",
    "logger.info(\"Saved trained model to %s\", MODEL_OUT_PATH)\n",
    "\n",
    "# Test encoding and prediction\n",
    "logger.info(\"One-hot encoding test sequences (seq_len=%d)\", SEQ_LEN)\n",
    "seq_e_test, stru_e_test = one_hot_encode(e_test, seqcol=\"sequence\", strucol=\"structure_full\", seq_len=SEQ_LEN)\n",
    "\n",
    "logger.info(\"Predicting on test set\")\n",
    "preds_scaled = model.predict([stru_e_test, seq_e_test, scaled_test_meta]).reshape(-1)\n",
    "preds_orig = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "e_test = e_test.copy()\n",
    "e_test[\"pred\"] = preds_orig\n",
    "\n",
    "# Evaluation metrics\n",
    "r2 = r2_score_from_slope(e_test[\"rl\"].values, e_test[\"pred\"].values)\n",
    "spearman_corr, spearman_p = spearmanr(e_test[\"rl\"].values, e_test[\"pred\"].values)\n",
    "mse = ((e_test[\"rl\"].values - e_test[\"pred\"].values) ** 2).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "logger.info(\"Evaluation results on test set:\")\n",
    "logger.info(\"  r-squared (linear regression) = %.4f\", r2)\n",
    "logger.info(\"  Spearman rho = %.4f (p=%.3e)\", spearman_corr, spearman_p)\n",
    "logger.info(\"  MSE = %.6f, RMSE = %.6f\", mse, rmse)\n",
    "\n",
    "# Show some example predictions\n",
    "e_test[[\"rl\",\"pred\"]].head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
