{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2782e816",
   "metadata": {},
   "source": [
    "# eGFP varying-length training notebook\n",
    "\n",
    "Converted from `train_egfp_varying_length.py`. This notebook walks through the entire pipeline to train a CNN + BiLSTM + Attention model on eGFP sequences of varying lengths (25â€“100 nt) padded/truncated to 130 nt.\n",
    "\n",
    "**Author:** Mike Wang\n",
    "\n",
    "**Notes:**\n",
    "- Update DATA_PATH and MODEL_OUT_PATH in the Configuration cell before running.\n",
    "- Training can take long; use smaller subsets or fewer epochs for quick debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835c323",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit file paths and hyperparameters below to match your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / constants\n",
    "DATA_PATH = r\"/home/sandbox/data/GSM4084997_varying_length_25to100_structure_feature_table_maxBPspan_30.txt\"\n",
    "MODEL_OUT_PATH = r\"/home/sandbox/models/synthetic/model_eGFP_25_100.h5\"\n",
    "RANDOM_STATE = 3407\n",
    "MAX_PAD_LEN = 130        # final fixed input length\n",
    "SAMPLE_PER_LEN = 200     # per-length sampling for balanced test set\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 999\n",
    "\n",
    "# DATA_PATH, MODEL_OUT_PATH, MAX_PAD_LEN, SAMPLE_PER_LEN, BATCH_SIZE, EPOCHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f96ae",
   "metadata": {},
   "source": [
    "## Imports and environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & logging\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82821573",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Functions for test-set construction, padding and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f062cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: build balanced test set, padding, encoding helpers\n",
    "\n",
    "def build_test_set_by_length(df: pd.DataFrame, len_col: str = \"len\", sample_per_len: int = SAMPLE_PER_LEN) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for length in sorted(df[len_col].unique()):\n",
    "        subset = df[df[len_col] == length]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        parts.append(subset.iloc[:sample_per_len].copy())\n",
    "    if parts:\n",
    "        test_df = pd.concat(parts, ignore_index=True)\n",
    "    else:\n",
    "        test_df = pd.DataFrame(columns=df.columns)\n",
    "    return test_df\n",
    "\n",
    "def pad_right_or_left_to_fixed(seq: str, total_len: int = MAX_PAD_LEN, pad_char: str = \"N\", from_right: bool = True) -> str:\n",
    "    if seq is None:\n",
    "        seq = \"\"\n",
    "    seq = str(seq)\n",
    "    if len(seq) >= total_len:\n",
    "        return seq[-total_len:]\n",
    "    pad_needed = total_len - len(seq)\n",
    "    if from_right:\n",
    "        return pad_char * pad_needed + seq\n",
    "    else:\n",
    "        return seq + pad_char * pad_needed\n",
    "\n",
    "def one_hot_encode(df: pd.DataFrame, seqcol: str, strucol: str, seq_len: int = MAX_PAD_LEN) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    nuc_map: Dict[str, tuple] = { \"A\": (1,0,0,0), \"C\": (0,1,0,0), \"G\": (0,0,1,0), \"T\": (0,0,0,1), \"U\": (0,0,0,1), \"N\": (0,0,0,0) }\n",
    "    bases = [\"A\",\"C\",\"G\",\"T\"]\n",
    "    struct_symbols = [\"(\",\")\",\".\"]\n",
    "    stru_map: Dict[str, list] = {}\n",
    "    idx = 0\n",
    "    for b in bases:\n",
    "        for s in struct_symbols:\n",
    "            vec = [0]*12\n",
    "            vec[idx] = 1\n",
    "            stru_map[f\"{b}{s}\"] = vec\n",
    "            idx += 1\n",
    "    fallback_12 = [0]*12\n",
    "\n",
    "    N = len(df)\n",
    "    seq_out = np.zeros((N, seq_len, 4), dtype=np.float32)\n",
    "    stru_out = np.zeros((N, seq_len, 12), dtype=np.float32)\n",
    "\n",
    "    seq_series = df[seqcol].fillna(\"\").astype(str).str.upper()\n",
    "    stru_series = df[strucol].fillna(\"\").astype(str).str.upper()\n",
    "\n",
    "    for i, (sseq, sstru) in enumerate(zip(seq_series, stru_series)):\n",
    "        if len(sseq) < seq_len:\n",
    "            sseq = pad_right_or_left_to_fixed(sseq, seq_len, pad_char=\"N\", from_right=True)\n",
    "        if len(sstru) < seq_len:\n",
    "            sstru = pad_right_or_left_to_fixed(sstru, seq_len, pad_char=\"N\", from_right=True)\n",
    "        for j, ch in enumerate(sseq[:seq_len]):\n",
    "            seq_out[i, j, :] = nuc_map.get(ch, (0,0,0,0))\n",
    "        for j, (bch, sch) in enumerate(zip(sseq[:seq_len], sstru[:seq_len])):\n",
    "            key = f\"{bch}{sch}\"\n",
    "            stru_out[i, j, :] = stru_map.get(key, fallback_12)\n",
    "    return seq_out, stru_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03251f",
   "metadata": {},
   "source": [
    "## Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, bias: bool = True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(\"glorot_uniform\")\n",
    "        self.bias = bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],), initializer=self.init, name=f\"{self.name}_W\")\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(1,), initializer=\"zeros\", name=f\"{self.name}_b\")\n",
    "        else:\n",
    "            self.b = None\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features = tf.shape(x)[-1]\n",
    "        flat_x = tf.reshape(x, (-1, features))\n",
    "        e = tf.reshape(tf.matmul(flat_x, tf.reshape(self.W, (features, 1))), (-1, tf.shape(x)[1]))\n",
    "        if self.bias:\n",
    "            e = e + self.b\n",
    "        e = tf.tanh(e)\n",
    "        a = tf.exp(e)\n",
    "        if mask is not None:\n",
    "            a = a * tf.cast(mask, dtype=tf.float32)\n",
    "        denom = tf.reduce_sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon()\n",
    "        a = a / denom\n",
    "        a_exp = tf.expand_dims(a, axis=-1)\n",
    "        context = tf.reduce_sum(a_exp * x, axis=1)\n",
    "        return context, a_exp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2426b",
   "metadata": {},
   "source": [
    "## Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model builder (structure + sequence streams with attention)\n",
    "def build_model_with_attention(maxlen: int = MAX_PAD_LEN, hidden_dim: int = 64, dropout: float = 0.3) -> tf.keras.Model:\n",
    "    input_stru = L.Input(shape=(maxlen, 12), name='Input_stru')\n",
    "    input_seq = L.Input(shape=(maxlen, 4), name='Input_seq')\n",
    "    input_mfe = L.Input(shape=(1,), name='Input_mfe')\n",
    "\n",
    "    # mask creation\n",
    "    make_mask = L.Lambda(lambda x: tf.reduce_any(tf.not_equal(x, 0), axis=-1), name='make_mask')\n",
    "    mask_bool = make_mask(input_seq)\n",
    "    mask_expanded = L.Lambda(lambda m: tf.expand_dims(tf.cast(m, dtype=tf.float32), axis=-1), name='mask_expand')(mask_bool)\n",
    "\n",
    "    # structure branch\n",
    "    s = L.Conv1D(32, 1, padding='same', name='Conv1D_stru')(input_stru)\n",
    "    s = s * mask_expanded\n",
    "    s = L.BatchNormalization()(s)\n",
    "    s = L.ReLU()(s)\n",
    "\n",
    "    s1 = L.Conv1D(64, 7, padding='same', name='Conv1D_stru_f1')(s)\n",
    "    s1 = s1 * mask_expanded\n",
    "    s1 = L.BatchNormalization()(s1)\n",
    "    s1 = L.ReLU()(s1)\n",
    "\n",
    "    s2 = L.Conv1D(64, 9, padding='same', name='Conv1D_stru_f2')(s)\n",
    "    s2 = s2 * mask_expanded\n",
    "    s2 = L.BatchNormalization()(s2)\n",
    "    s2 = L.ReLU()(s2)\n",
    "\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s1')(s1)\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s1_2')(lstm_s1)\n",
    "\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s2')(s2)\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s2_2')(lstm_s2)\n",
    "\n",
    "    merged_s = L.concatenate([lstm_s1, lstm_s2], axis=-1)\n",
    "    merged_s = L.Conv1D(256, 1, padding='same', name='Conv1D_stru_merged')(merged_s)\n",
    "    merged_s = merged_s * mask_expanded\n",
    "    merged_s = L.BatchNormalization()(merged_s)\n",
    "    merged_s = L.ReLU()(merged_s)\n",
    "\n",
    "    stru_out, stru_att = Attention(name='Attention_stru')(merged_s, mask=mask_bool)\n",
    "    stru_out = L.Dropout(0.3)(stru_out)\n",
    "\n",
    "    # sequence branch\n",
    "    q = L.Conv1D(32, 1, padding='same', name='Conv1D_seq')(input_seq)\n",
    "    q = q * mask_expanded\n",
    "    q = L.BatchNormalization()(q)\n",
    "    q = L.ReLU()(q)\n",
    "\n",
    "    q1 = L.Conv1D(64, 3, padding='same', name='Conv1D_seq_f1')(q)\n",
    "    q1 = q1 * mask_expanded\n",
    "    q1 = L.BatchNormalization()(q1)\n",
    "    q1 = L.ReLU()(q1)\n",
    "\n",
    "    q2 = L.Conv1D(64, 5, padding='same', name='Conv1D_seq_f2')(q)\n",
    "    q2 = q2 * mask_expanded\n",
    "    q2 = L.BatchNormalization()(q2)\n",
    "    q2 = L.ReLU()(q2)\n",
    "\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q1')(q1)\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q1_2')(lstm_q1)\n",
    "\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q2')(q2)\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q2_2')(lstm_q2)\n",
    "\n",
    "    merged_q = L.concatenate([lstm_q1, lstm_q2], axis=-1)\n",
    "    merged_q = L.Conv1D(256, 1, padding='same', name='Conv1D_seq_merged')(merged_q)\n",
    "    merged_q = merged_q * mask_expanded\n",
    "    merged_q = L.BatchNormalization()(merged_q)\n",
    "    merged_q = L.ReLU()(merged_q)\n",
    "\n",
    "    seq_out, seq_att = Attention(name='Attention_seq')(merged_q, mask=mask_bool)\n",
    "    seq_out = L.Dropout(0.3)(seq_out)\n",
    "\n",
    "    # head\n",
    "    x = L.Concatenate()([stru_out, seq_out, input_mfe])\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.5)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    out = L.Dense(1, activation='linear', name='out')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_stru, input_seq, input_mfe], outputs=out)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95374054",
   "metadata": {},
   "source": [
    "## Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "def main() -> Dict[str, Any]:\n",
    "    logger.info(\"Loading data from %s\", DATA_PATH)\n",
    "    df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "    df['len'] = df['sequence'].astype(str).apply(len)\n",
    "\n",
    "    # Build balanced test set\n",
    "    e_test = build_test_set_by_length(df, len_col='len', sample_per_len=SAMPLE_PER_LEN)\n",
    "    logger.info(\"Constructed test set with %d examples\", len(e_test))\n",
    "\n",
    "    # Training set = df \\ e_test\n",
    "    if len(e_test) > 0:\n",
    "        test_idx = set(e_test.index)\n",
    "        e_train = df[~df.index.isin(test_idx)].copy()\n",
    "    else:\n",
    "        e_train = df.copy()\n",
    "\n",
    "    # Pad sequences to fixed length (keep rightmost)\n",
    "    e_train['seq130'] = e_train['sequence'].astype(str).apply(lambda s: pad_right_or_left_to_fixed(s, total_len=MAX_PAD_LEN, from_right=True))\n",
    "    e_train['stru130'] = e_train['structure_full'].astype(str).apply(lambda s: pad_right_or_left_to_fixed(s, total_len=MAX_PAD_LEN, from_right=True))\n",
    "    e_test['seq130'] = e_test['sequence'].astype(str).apply(lambda s: pad_right_or_left_to_fixed(s, total_len=MAX_PAD_LEN, from_right=True))\n",
    "    e_test['stru130'] = e_test['structure_full'].astype(str).apply(lambda s: pad_right_or_left_to_fixed(s, total_len=MAX_PAD_LEN, from_right=True))\n",
    "\n",
    "    # One-hot encode training set\n",
    "    logger.info(\"One-hot encoding training sequences (N=%d)\", len(e_train))\n",
    "    train_seq_onehot, train_stru_onehot = one_hot_encode(e_train, seqcol='seq130', strucol='stru130', seq_len=MAX_PAD_LEN)\n",
    "\n",
    "    # Meta (Norm_mfe) and scaling\n",
    "    e_train['Norm_mfe'] = e_train['MFE_full'] / e_train['len']\n",
    "    e_test['Norm_mfe'] = e_test['MFE_full'] / e_test['len']\n",
    "\n",
    "    mm = MinMaxScaler(feature_range=(0,1))\n",
    "    train_meta = mm.fit_transform(e_train[['Norm_mfe']].values)\n",
    "    test_meta = mm.transform(e_test[['Norm_mfe']].values) if len(e_test) > 0 else np.zeros((0,1))\n",
    "\n",
    "    # target scaling on training set\n",
    "    target_scaler = StandardScaler()\n",
    "    e_train['scaled_rl'] = target_scaler.fit_transform(e_train['rl'].values.reshape(-1,1)).flatten()\n",
    "\n",
    "    # Build model and summary\n",
    "    model = build_model_with_attention(maxlen=MAX_PAD_LEN)\n",
    "    model.summary(print_fn=lambda s: logger.info(s))\n",
    "\n",
    "    # Train/validation split on training set\n",
    "    X_seq_train, X_seq_val, X_stru_train, X_stru_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
    "        train_seq_onehot, train_stru_onehot, train_meta, e_train['scaled_rl'].values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        [X_stru_train, X_seq_train, X_meta_train],\n",
    "        y_train,\n",
    "        validation_data=([X_stru_val, X_seq_val, X_meta_val], y_val),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(os.path.dirname(MODEL_OUT_PATH), exist_ok=True)\n",
    "    model.save(MODEL_OUT_PATH)\n",
    "    logger.info(\"Saved model to %s\", MODEL_OUT_PATH)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if len(e_test) > 0:\n",
    "        logger.info(\"Encoding and predicting on test set (N=%d)\", len(e_test))\n",
    "        test_seq_onehot, test_stru_onehot = one_hot_encode(e_test, seqcol='seq130', strucol='stru130', seq_len=MAX_PAD_LEN)\n",
    "        preds_scaled = model.predict([test_stru_onehot, test_seq_onehot, test_meta]).reshape(-1)\n",
    "        preds = target_scaler.inverse_transform(preds_scaled.reshape(-1,1)).reshape(-1)\n",
    "        e_test = e_test.copy()\n",
    "        e_test['pred'] = preds\n",
    "        r2 = r2_from_linreg(e_test['rl'].values, e_test['pred'].values)\n",
    "        from scipy.stats import spearmanr\n",
    "        rho, pval = spearmanr(e_test['rl'].values, e_test['pred'].values)\n",
    "        mse = ((e_test['rl'].values - e_test['pred'].values) ** 2).mean()\n",
    "        rmse = np.sqrt(mse)\n",
    "        logger.info(\"Test evaluation: r2=%.4f, spearman rho=%.4f (p=%.3e), MSE=%.6f, RMSE=%.6f\", r2, rho, pval, mse, rmse)\n",
    "    else:\n",
    "        logger.warning(\"Test set empty, skipped evaluation\")\n",
    "\n",
    "    return {'model': model, 'history': history, 'train_df': e_train, 'test_df': e_test, 'meta_scaler': mm, 'target_scaler': target_scaler}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training pipeline (comment out if running cells interactively step-by-step)\n",
    "# results = main()\n",
    "# results['test_df'].head()\n",
    "print('Notebook prepared. To run full training call `main()` (be cautious: long-running).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
