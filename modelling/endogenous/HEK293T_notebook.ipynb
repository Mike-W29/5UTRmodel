{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8f4b95",
   "metadata": {},
   "source": [
    "# HEK293T endogenous model training\n",
    "\n",
    "**Notebook**: converted from `HEK293T.py` â€” contains the full pipeline to train a sequence+structure model (CNN + BiLSTM + Attention) on HEK293T endogenous data. The notebook is annotated with detailed explanations for each step (data loading, preprocessing, model, training, evaluation) so you can run interactively, inspect intermediate results, and iterate.\n",
    "\n",
    "**Author:** Mike Wang \n",
    "\n",
    "**Notes:**\n",
    "- Before running, update the file paths in the **Configuration** cell to match your environment.\n",
    "- This notebook uses TensorFlow 2.x, scikit-learn, pandas, numpy. Ensure the environment has these packages.\n",
    "- Long-running steps (model training) may take hours depending on data and hardware; you can run smaller subsets for quick checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99202924",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit the file paths and hyperparameters below to match your environment before executing cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / constants\n",
    "DATA_COUNTS_PATH = r\"/home/sandbox/data/endogenous/df_counts_and_len.TE_sorted.HEK_Andrev2015.with_annot.txt\"\n",
    "TRAIN_SEQ_PATH = r\"/home/sandbox/data/endogenous/Homo_trian_seq_feature_table_maxBPspan30.txt\"\n",
    "MODEL_OUT_PATH = r\"/home/sandbox/models/endogenous/endogenesis_model_HKE293T.h5\"\n",
    "\n",
    "# Seeds and reproducibility\n",
    "RANDOM_STATE = 3407\n",
    "TF_SEED = RANDOM_STATE\n",
    "NP_SEED = RANDOM_STATE\n",
    "PY_RANDOM_SEED = RANDOM_STATE\n",
    "\n",
    "# Filtering thresholds and model params\n",
    "MIN_RNA_RPKM = 1.0\n",
    "MIN_RIBO_RPKM = 1.0\n",
    "MAX_LEN = 330      # fixed input length (pad/truncate to rightmost MAX_LEN bases)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 999\n",
    "TEST_SPLIT_RATIO = 0.10  # fraction reserved for test set\n",
    "\n",
    "# Print configuration summary\n",
    "DATA_COUNTS_PATH, TRAIN_SEQ_PATH, MODEL_OUT_PATH, MAX_LEN, BATCH_SIZE, EPOCHS, TEST_SPLIT_RATIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cec5b8",
   "metadata": {},
   "source": [
    "## Imports and environment setup\n",
    "Load required libraries and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Seeds\n",
    "tf.random.set_seed(TF_SEED)\n",
    "np.random.seed(NP_SEED)\n",
    "random.seed(PY_RANDOM_SEED)\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3885a",
   "metadata": {},
   "source": [
    "## Data loading and filtering\n",
    "Load expression counts and UTR annotations, filter low-expression transcripts, and merge tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expression counts (ensure the file exists at DATA_COUNTS_PATH)\n",
    "logger.info(\"Loading expression counts & annotations from %s\", DATA_COUNTS_PATH)\n",
    "df_counts = pd.read_csv(DATA_COUNTS_PATH, sep=\" \", index_col=0)\n",
    "\n",
    "# Basic filter by expression thresholds to remove very lowly expressed transcripts\n",
    "logger.info(\"Filtering transcripts with rpkm_rnaseq > %s and rpkm_riboseq > %s\", MIN_RNA_RPKM, MIN_RIBO_RPKM)\n",
    "df_counts = df_counts[(df_counts[\"rpkm_rnaseq\"] > MIN_RNA_RPKM) & (df_counts[\"rpkm_riboseq\"] > MIN_RIBO_RPKM)]\n",
    "logger.info(\"After expression filter: %d rows remain\", len(df_counts))\n",
    "\n",
    "# Load sequence feature table (UTR sequences and structure)\n",
    "logger.info(\"Loading sequence feature table from %s\", TRAIN_SEQ_PATH)\n",
    "seq_table = pd.read_csv(TRAIN_SEQ_PATH, sep=\"\\t\")\n",
    "seq_table['len'] = seq_table['utr'].astype(str).apply(len)\n",
    "\n",
    "# Keep only transcripts present in both tables\n",
    "seq_filtered = seq_table[seq_table['Transcript_ID'].isin(df_counts['ensembl_tx_id'])]\n",
    "df_filtered = df_counts[df_counts['ensembl_tx_id'].isin(seq_filtered['Transcript_ID'])]\n",
    "\n",
    "# Merge the two tables on transcript ID\n",
    "df_merged = pd.merge(\n",
    "    df_filtered,\n",
    "    seq_filtered[['Transcript_ID','utr','Sequence','MFE_UTR','structure_UTR','MFE_full','structure_full']],\n",
    "    left_on='ensembl_tx_id', right_on='Transcript_ID', how='left'\n",
    ")\n",
    "\n",
    "# Compute log TE (target)\n",
    "df_merged['log_te'] = np.log(df_merged['te'])\n",
    "df_merged['len'] = df_merged['utr'].astype(str).apply(len)\n",
    "\n",
    "# Filter UTR length range (keep 25..MAX_LEN)\n",
    "df_merged = df_merged[(df_merged['len'] >= 25) & (df_merged['len'] <= MAX_LEN)]\n",
    "logger.info(\"After merging & length filter: %d rows\", len(df_merged))\n",
    "\n",
    "# For transcripts with multiple isoforms, keep the isoform with maximum TE (as in the original script)\n",
    "idx = df_merged.groupby('Sequence')['te'].idxmax()\n",
    "df_merged = df_merged.loc[idx].reset_index(drop=True)\n",
    "logger.info(\"After isoform deduplication: %d rows\", len(df_merged))\n",
    "\n",
    "# Show a quick summary\n",
    "df_merged[['Sequence','len','te','log_te']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167cc95",
   "metadata": {},
   "source": [
    "## Train/Test split and sequence padding\n",
    "Randomly split into train/test, pad sequences/structures to fixed length (keep rightmost bases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices into train/test (randomized reproducibly)\n",
    "indices = df_merged.index.to_list()\n",
    "random.Random(RANDOM_STATE).shuffle(indices)\n",
    "split_at = int(len(indices) * (1.0 - TEST_SPLIT_RATIO))\n",
    "train_idx, test_idx = indices[:split_at], indices[split_at:]\n",
    "e_train = df_merged.loc[train_idx].reset_index(drop=True)\n",
    "e_test = df_merged.loc[test_idx].reset_index(drop=True)\n",
    "logger.info(\"Train / Test sizes: %d / %d\", len(e_train), len(e_test))\n",
    "\n",
    "# Helper to pad sequences keeping rightmost MAX_LEN bases\n",
    "def pad_keep_right_local(seq: str, total_len: int = MAX_LEN, pad_char: str = 'N') -> str:\n",
    "    if seq is None:\n",
    "        seq = ''\n",
    "    s = str(seq)\n",
    "    if len(s) >= total_len:\n",
    "        return s[-total_len:]\n",
    "    return pad_char * (total_len - len(s)) + s\n",
    "\n",
    "# Add padded columns used for model input\n",
    "e_train['seq330'] = e_train['Sequence'].astype(str).apply(lambda s: pad_keep_right_local(s, MAX_LEN))\n",
    "e_train['stru330'] = e_train['structure_full'].astype(str).apply(lambda s: pad_keep_right_local(s, MAX_LEN))\n",
    "e_test['seq330'] = e_test['Sequence'].astype(str).apply(lambda s: pad_keep_right_local(s, MAX_LEN))\n",
    "e_test['stru330'] = e_test['structure_full'].astype(str).apply(lambda s: pad_keep_right_local(s, MAX_LEN))\n",
    "\n",
    "# Show first rows\n",
    "e_train[['Sequence','seq330']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fa496",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "Convert sequences and structure strings into numerical arrays the model can ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_dataframe(df, seqcol='seq330', strucol='stru330', seq_len=MAX_LEN):\n",
    "    nuc_map = {'A': (1,0,0,0), 'C': (0,1,0,0), 'G': (0,0,1,0), 'T': (0,0,0,1), 'U': (0,0,0,1), 'N': (0,0,0,0)}\n",
    "    bases = ['A','C','G','T']\n",
    "    struct_symbols = ['(',')','.']\n",
    "    stru_map = {}\n",
    "    idx = 0\n",
    "    for b in bases:\n",
    "        for s in struct_symbols:\n",
    "            v = [0]*12\n",
    "            v[idx] = 1\n",
    "            stru_map[f\"{b}{s}\"] = v\n",
    "            idx += 1\n",
    "    fallback_12 = [0]*12\n",
    "\n",
    "    N = len(df)\n",
    "    seq_feat = np.zeros((N, seq_len, 4), dtype=np.float32)\n",
    "    stru_feat = np.zeros((N, seq_len, 12), dtype=np.float32)\n",
    "\n",
    "    seq_series = df[seqcol].fillna('').astype(str).str.upper()\n",
    "    stru_series = df[strucol].fillna('').astype(str).str.upper()\n",
    "\n",
    "    for i, (sseq, sstru) in enumerate(zip(seq_series, stru_series)):\n",
    "        if len(sseq) < seq_len:\n",
    "            sseq = pad_keep_right_local(sseq, seq_len)\n",
    "        if len(sstru) < seq_len:\n",
    "            sstru = pad_keep_right_local(sstru, seq_len)\n",
    "        sseq = sseq[:seq_len]\n",
    "        sstru = sstru[:seq_len]\n",
    "        for j, ch in enumerate(sseq):\n",
    "            seq_feat[i, j, :] = nuc_map.get(ch, (0,0,0,0))\n",
    "        for j, (bch, sch) in enumerate(zip(sseq, sstru)):\n",
    "            stru_feat[i, j, :] = stru_map.get(f\"{bch}{sch}\", fallback_12)\n",
    "\n",
    "    return seq_feat, stru_feat\n",
    "\n",
    "# Run encoding on training set (this can be memory heavy for large datasets)\n",
    "train_seq_onehot, train_stru_onehot = one_hot_encode_dataframe(e_train, seqcol='seq330', strucol='stru330', seq_len=MAX_LEN)\n",
    "train_seq_onehot.shape, train_stru_onehot.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea72e4",
   "metadata": {},
   "source": [
    "## Metadata scaling (MFE normalization)\n",
    "Compute per-length normalized MFE and fit a MinMax scaler on training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized MFE per sequence length\n",
    "e_train['Norm_mfe'] = e_train['MFE_full'] / e_train['len']\n",
    "e_test['Norm_mfe'] = e_test['MFE_full'] / e_test['len']\n",
    "\n",
    "meta_cols = ['Norm_mfe']\n",
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "train_meta = mm.fit_transform(e_train[meta_cols].values)\n",
    "test_meta = mm.transform(e_test[meta_cols].values) if len(e_test) > 0 else np.zeros((0, len(meta_cols)))\n",
    "\n",
    "train_meta.shape, test_meta.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ae9e5",
   "metadata": {},
   "source": [
    "## Target scaling and model construction\n",
    "Scale log-TE with StandardScaler fitted on training data, then build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale target on training set\n",
    "target_scaler = StandardScaler()\n",
    "e_train['scaled_log_te'] = target_scaler.fit_transform(e_train['log_te'].values.reshape(-1,1)).flatten()\n",
    "\n",
    "# Define custom Attention layer (same implementation as in script)\n",
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Attention layer returning (context_vector, attention_weights).\n",
    "    \"\"\"\n",
    "    def __init__(self, bias: bool = True, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(\"glorot_uniform\")\n",
    "        self.bias = bias\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.features_dim = 0\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],), initializer=self.init, name=f\"{self.name}_W\",\n",
    "                                 regularizer=self.W_regularizer, constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(1,), initializer=\"zeros\", name=f\"{self.name}_b\",\n",
    "                                     regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = tf.shape(x)[1]\n",
    "        flat_x = tf.reshape(x, (-1, features_dim))\n",
    "        e = tf.reshape(tf.matmul(flat_x, tf.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            e = e + self.b\n",
    "        e = tf.tanh(e)\n",
    "        a = tf.exp(e)\n",
    "        if mask is not None:\n",
    "            a *= tf.cast(mask, tf.float32)\n",
    "        a /= tf.cast(tf.reduce_sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(), tf.float32)\n",
    "        a_expanded = tf.expand_dims(a, axis=-1)\n",
    "        c = tf.reduce_sum(a_expanded * x, axis=1)\n",
    "        return c, a_expanded\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.features_dim)\n",
    "\n",
    "\n",
    "# Build model function (same architecture as provided)\n",
    "def build_model_with_attention(maxlen: int = MAX_LEN, hidden_dim: int = 64, dropout: float = 0.3) -> tf.keras.Model:\n",
    "    input_stru = L.Input(shape=(maxlen, 12), name='Input_stru')\n",
    "    input_seq = L.Input(shape=(maxlen, 4), name='Input_seq')\n",
    "    input_mfe = L.Input(shape=(1,), name='Input_mfe')\n",
    "\n",
    "    mask_bool = L.Lambda(lambda x: tf.reduce_any(tf.not_equal(x,0), axis=-1), name='make_mask')(input_seq)\n",
    "    mask_exp = L.Lambda(lambda m: tf.expand_dims(tf.cast(m, tf.float32), axis=-1), name='mask_expand')(mask_bool)\n",
    "\n",
    "    # Structure branch\n",
    "    s = L.Conv1D(32,1,padding='same', name='Conv1D_stru')(input_stru)\n",
    "    s = s * mask_exp\n",
    "    s = L.BatchNormalization()(s)\n",
    "    s = L.ReLU()(s)\n",
    "    s1 = L.Conv1D(64,7,padding='same', name='Conv1D_stru_f1')(s)\n",
    "    s1 = s1 * mask_exp\n",
    "    s1 = L.BatchNormalization()(s1)\n",
    "    s1 = L.ReLU()(s1)\n",
    "    s2 = L.Conv1D(64,9,padding='same', name='Conv1D_stru_f2')(s)\n",
    "    s2 = s2 * mask_exp\n",
    "    s2 = L.BatchNormalization()(s2)\n",
    "    s2 = L.ReLU()(s2)\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s1')(s1)\n",
    "    lstm_s1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s1_2')(lstm_s1)\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s2')(s2)\n",
    "    lstm_s2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_s2_2')(lstm_s2)\n",
    "    merged_s = L.concatenate([lstm_s1, lstm_s2], axis=-1)\n",
    "    merged_s = L.Conv1D(256,1,padding='same', name='Conv1D_stru_merged')(merged_s)\n",
    "    merged_s = merged_s * mask_exp\n",
    "    merged_s = L.BatchNormalization()(merged_s)\n",
    "    merged_s = L.ReLU()(merged_s)\n",
    "    stru_out, stru_att = Attention(name='Attention_stru')(merged_s, mask=mask_bool)\n",
    "    stru_out = L.Dropout(0.3)(stru_out)\n",
    "\n",
    "    # Sequence branch\n",
    "    q = L.Conv1D(32,1,padding='same', name='Conv1D_seq')(input_seq)\n",
    "    q = q * mask_exp\n",
    "    q = L.BatchNormalization()(q)\n",
    "    q = L.ReLU()(q)\n",
    "    q1 = L.Conv1D(64,3,padding='same', name='Conv1D_seq_f1')(q)\n",
    "    q1 = q1 * mask_exp\n",
    "    q1 = L.BatchNormalization()(q1)\n",
    "    q1 = L.ReLU()(q1)\n",
    "    q2 = L.Conv1D(64,5,padding='same', name='Conv1D_seq_f2')(q)\n",
    "    q2 = q2 * mask_exp\n",
    "    q2 = L.BatchNormalization()(q2)\n",
    "    q2 = L.ReLU()(q2)\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q1')(q1)\n",
    "    lstm_q1 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q1_2')(lstm_q1)\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q2')(q2)\n",
    "    lstm_q2 = L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'), name='LSTM_q2_2')(lstm_q2)\n",
    "    merged_q = L.concatenate([lstm_q1, lstm_q2], axis=-1)\n",
    "    merged_q = L.Conv1D(256,1,padding='same', name='Conv1D_seq_merged')(merged_q)\n",
    "    merged_q = merged_q * mask_exp\n",
    "    merged_q = L.BatchNormalization()(merged_q)\n",
    "    merged_q = L.ReLU()(merged_q)\n",
    "    seq_out, seq_att = Attention(name='Attention_seq')(merged_q, mask=mask_bool)\n",
    "    seq_out = L.Dropout(0.3)(seq_out)\n",
    "\n",
    "    x = L.Concatenate()([stru_out, seq_out, input_mfe])\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "    x = L.Dropout(0.5)(x)\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    out = L.Dense(1, activation='linear', name='out')(x)\n",
    "    model = tf.keras.Model(inputs=[input_stru, input_seq, input_mfe], outputs=out)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Instantiate model (weights random until trained)\n",
    "model = build_model_with_attention(maxlen=MAX_LEN)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba8c45",
   "metadata": {},
   "source": [
    "## Training\n",
    "Split training set into train/validation, then train with EarlyStopping. Beware of runtime/compute costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0308b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation splits (on training set)\n",
    "X_seq_train, X_seq_val, X_stru_train, X_stru_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
    "    train_seq_onehot, train_stru_onehot, train_meta, e_train['scaled_log_te'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_stru_train, X_seq_train, X_meta_train],\n",
    "    y_train,\n",
    "    validation_data=([X_stru_val, X_seq_val, X_meta_val], y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Show training history keys\n",
    "history.history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf520b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63657ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2b9df67",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Save the trained model to disk. Ensure the parent directory exists or is writable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(MODEL_OUT_PATH), exist_ok=True)\n",
    "model.save(MODEL_OUT_PATH)\n",
    "logger.info(\"Saved trained model to %s\", MODEL_OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f9922",
   "metadata": {},
   "source": [
    "## Evaluation on test set\n",
    "Encode test set, predict, inverse-transform targets and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(e_test) > 0:\n",
    "    logger.info(\"Encoding and predicting on test set (N=%d)\", len(e_test))\n",
    "    test_seq_onehot, test_stru_onehot = one_hot_encode_dataframe(e_test, seqcol='seq330', strucol='stru330', seq_len=MAX_LEN)\n",
    "    preds_scaled = model.predict([test_stru_onehot, test_seq_onehot, test_meta]).reshape(-1)\n",
    "    preds = target_scaler.inverse_transform(preds_scaled.reshape(-1,1)).reshape(-1)\n",
    "    e_test = e_test.copy()\n",
    "    e_test['pred_log_te'] = preds\n",
    "\n",
    "    # Metrics\n",
    "    from scipy.stats import linregress, spearmanr\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(e_test['log_te'].values, e_test['pred_log_te'].values)\n",
    "    r2 = r_value ** 2\n",
    "    rho, pval = spearmanr(e_test['log_te'].values, e_test['pred_log_te'].values)\n",
    "    mse = ((e_test['log_te'].values - e_test['pred_log_te'].values) ** 2).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    logger.info(\"Test metrics: r2=%.4f, spearman rho=%.4f (p=%.3e), MSE=%.6f, RMSE=%.6f\", r2, rho, pval, mse, rmse)\n",
    "else:\n",
    "    logger.warning(\"Test set empty: skipping evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ca6bc",
   "metadata": {},
   "source": [
    "## Save scalers and return objects\n",
    "Save fitted scalers for later inference and export key objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b98cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers for inference later\n",
    "scaler_dir = os.path.join(os.path.dirname(MODEL_OUT_PATH), 'scalers')\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(mm, os.path.join(scaler_dir, 'meta_mm_scaler.pkl'))\n",
    "joblib.dump(target_scaler, os.path.join(scaler_dir, 'target_standard_scaler.pkl'))\n",
    "logger.info(\"Saved scalers in %s\", scaler_dir)\n",
    "\n",
    "# Expose variables for interactive use (Jupyter cell outputs)\n",
    "model, history, e_train, e_test, mm, target_scaler\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
